---
title: 动手学深度学习-逻辑回归精简实现
date: 2021-04-18 15:03:23
categories:
- [编程, 深度学习]
tags:
- 深度学习
- 逻辑回归
---



## 搭建简单模型的一般框架

```python
import torch
from torch.utils import data
from torch import nn

dataset = data.TensorDataset(features, labels)
data_iter = data.DataLoader(dataset, batch_size, shuffle=True)

model = nn.Sequential(nn.Linear(x, y), ...)
loss = nn.loss_fun_name()
trainer = nn.optim.optimizer_name(model.parameters(), lr=learning_rate)

for epoch in len(epoch_num):
	for X, y in data_iter:
        l = loss(model(X), y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    print(loss(model(features, labels)))
```

## 逻辑回归的实现

**features（二维）示例：**

| feature1(数值) | feature2（数值） |
| :------------: | :--------------: |
|      f11       |       f21        |
|      f12       |       f22        |
|      f13       |       f23        |
|      ...       |       ...        |

**labels示例（和每一条feature一一对应）：**

| label（数值） |
| :-----------: |
|      l1       |
|      l2       |
|      l3       |
|      ...      |



```python
# 数据加载
import torch
from torch.utils import data

# 根据预设的w和b，通过增添一定噪音，人工创造出100条数据集
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)

# 创建数据集，第一个参数为所有特征组成的Tensor，第二个是特征所对应标签组成的Tensor
dataset = data.TensorDataset(features, labels)
# 定义一个可迭代yeild数据的对象，每次yeild出batch_size个数据组，shuffle表示每个epoch否打乱数据（不明所以）
batch_size = 10
data_iter = data.DataLoader(dataset, batch_size, shuffle=True)

# 模型构建
from torch import nn

# 快速定义网络，只有一个线性层，相当于逻辑回归
net = nn.Sequential(nn.Linear(2, 1))

# 对第一层的网络的权重和偏差进行初始化
# weight数据符合mean=0, std=0.01的正态分布
net[0].wight.data.normal_(0, 0.01)
# bias数据置零
net[0].bias.data.fill(0)

# 定义损失函数
loss = nn.MESLoss()

# 定义优化器，此处为随机梯度下降
# 第一个参数为网络的参数，第二个参数为学习率
trainer = nn.optim.SGD(net.parameters(), lr=0.03)

# 开始训练
num_epochs = 3
for epoch in range(num_epochs):
	# data_iter每次产生batch_size数据，本循环即mini-batch算法
	for X, y in data_iter:
		# 算损失，第一个参数为传递数据的网络模型，第二个参数为目标标签
		l = loss(net(X), y)
		# 优化器梯度清零（默认是累加的）
		trainer.zero_grad()
		# 损失反向传播
		l.backward()
		# 优化器开始梯度下降
		trainer.step()
	# 一个epoch结束后，计算数据输入模型后目前的损失
	l = loss(net(features), labels)
	print("epoch: {:d}, loss: {:f}".format(epoch + 1, l))
```

